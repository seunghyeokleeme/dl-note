# Gradient Descent
GD Animation & learning rate (1, 0.1,0.01, 0.001)

## Learning Rate Comparison

The following animations show the effect of different learning rates on gradient descent optimization:

<table>
  <tr>
    <td><img src="gradient_descent_1.gif" alt="Learning Rate: 1.0" width="400"/></td>
    <td><img src="gradient_descent.gif" alt="Learning Rate: 0.1" width="400"/></td>
  </tr>
  <tr>
    <td><img src="gradient_descent_3.gif" alt="Learning Rate: 0.01" width="400"/></td>
    <td><img src="gradient_descent_2.gif" alt="Learning Rate: 0.001" width="400"/></td>
  </tr>
</table>

*Top left: Learning Rate = 1.0*  
*Top right: Learning Rate = 0.1*  
*Bottom left: Learning Rate = 0.01*  
*Bottom right: Learning Rate = 0.001*

As shown in the animations, the learning rate significantly affects the convergence speed and stability of the gradient descent algorithm. A high learning rate (1.0) may cause the algorithm to overshoot or diverge, while a very low learning rate (0.001) results in slow convergence. A moderate learning rate (0.1) often provides a good balance between convergence speed and stability.
